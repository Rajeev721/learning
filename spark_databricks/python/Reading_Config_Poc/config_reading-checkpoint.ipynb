{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "# Creating configParser Object\n",
    "\n",
    "cp = configparser.ConfigParser()\n",
    "\n",
    "# Reading the config files\n",
    "\n",
    "cp.read('D:\\Learning\\config_file_1.ini')\n",
    "\n",
    "#Assigning the parameter values\n",
    "\n",
    "path = cp.get('Paths', 'path')\n",
    "source_file1 = cp.get('Paths', 'source_file1')\n",
    "source_file2 = cp.get('Paths', 'source_file2')\n",
    "Target_file = cp.get('Paths', 'target_file')\n",
    "join_type = cp.get('DB','type_of_join')\n",
    "join_column = cp.get('DB','join_column').strip(\" \")\n",
    "database = cp.get('DB','Database').strip(\" \")\n",
    "target_table = cp.get('DB','target_table').strip(\" \")\n",
    "user_name = cp.get('DB','user_name').strip(\" \")\n",
    "password = cp.get('DB','password').strip(\" \")\n",
    "driver = cp.get('DB', 'driver').strip(\" \")\n",
    "connection_string = cp.get('DB','connection_string').strip(\" \")\n",
    "source_file1_path = os.path.join(path, source_file1)\n",
    "source_file2_path = os.path.join(path, source_file2)\n",
    "Target_file2_path = os.path.join(path, Target_file)\n",
    "df_write = cp.get('DB','df_write')\n",
    "config_table = cp.get('DB','config_table')\n",
    "server_name = cp.get('DB','server')\n",
    "sqldriver = cp.get('DB','sqldriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config file checkup\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ join_column + \" == source_df2.\"+ join_column+ \",\" +\"'\"+ join_type +\"')\"\n",
    "# a = \"source_df1.join(source_df2,source_df1.{0} == source_df2.{0},'{1}')\".format(join_column,join_type)\n",
    "\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "# b = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"{0}\").option(\"dbtable\",\"{1}\").option(\"user\",\"{2}\").option(\"password\",\"{3}\").option(\"driver\",\"{4}\").save()'.format(connection_string,target_table,user_name,password,driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Sepal_Length: double (nullable = true)\n",
      " |-- Sepal_Width: double (nullable = true)\n",
      " |-- Petal_Length: double (nullable = true)\n",
      " |-- Petal_Width: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)\n",
    "\n",
    "#a = join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\")\n",
    "#a.show()\n",
    "print(join_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "| ID|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|  6|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|  7|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|  8|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|  9|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "| 10|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90.save.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open server 'rajeev-sql-server' requested by the login. Client with IP address '157.47.103.230' is not allowed to access the server.  To enable access, use the Windows Azure Management Portal or run sp_set_firewall_rule on the master database to create a firewall rule for this IP address or address range.  It may take up to five minutes for this change to take effect. ClientConnectionId:6df9e89c-5432-4664-80ec-6830df7bf211\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\r\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:283)\r\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:129)\r\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5333)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:4066)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:4004)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7418)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2768)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2418)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2265)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1291)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:881)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c8c6311ec306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Following parameter is stored in config file to execute that we are using exec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df_write = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o90.save.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open server 'rajeev-sql-server' requested by the login. Client with IP address '157.47.103.230' is not allowed to access the server.  To enable access, use the Windows Azure Management Portal or run sp_set_firewall_rule on the master database to create a firewall rule for this IP address or address range.  It may take up to five minutes for this change to take effect. ClientConnectionId:6df9e89c-5432-4664-80ec-6830df7bf211\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\r\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:283)\r\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:129)\r\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5333)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:4066)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:4004)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7418)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2768)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2418)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2265)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1291)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:881)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "eval(b)\n",
    "\n",
    "# Following parameter is stored in config file to execute that we are using exec\n",
    "#df_write = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "#\n",
    "\n",
    "exec(df_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Uploading config details into Table'''\n",
    "df_config_load = spark.read.csv('D:\\Learning\\config_file_load.csv',header=True)\n",
    "df_config_load1 = df_config_load.select(col(\"ID_NUM\").cast('int'),\"SOURCE_PATH\",\"SOURCE_FILE_1\",\"SOURCE_FILE_2\",\"TARGET_FILE\",\"DATABASE_N\",\"CONNECTION_STRING\",\"DRIVER\",\"SOURCE_TABLES\",\"TARGET_TABLE\",\"TYPE_OF_JOIN\",\"JOIN_COLUMN\",\"USER_NAME_V\",\"PASSWORD_V\")\n",
    "df_config_load.printSchema()\n",
    "\n",
    "config_upload = 'df_config_load1.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "eval(config_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Reading Config details from Table '''\n",
    "config_load = 'spark.read.format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").load()'\n",
    "config_load_df = eval(config_load)\n",
    "config_load_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from config file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config Table\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect('DRIVER='+sqldriver+';SERVER='+server_name+';PORT=1433;DATABASE='+database+';UID='+user_name+';PWD='+ password)\n",
    "\n",
    "#'pyodbc.connect(\"DRIVER=\"{0}\";SERVER=\"{1}\";PORT=1433;DATABASE=\"{2}\";UID=\"{3}\";PWD=\"{4})'.format(sqldriver,server_name,database,user_name,password)\n",
    "\n",
    "\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('select * from SalesLT.config_table where ID_NUM = 1')\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_NUM = row[0]\n",
    "SOURCE_PATH = row[1]\n",
    "SOURCE_FILE_1 = row[2]\n",
    "SOURCE_FILE_2 = row[3]\n",
    "TARGET_FILE = row[4]\n",
    "DATABASE_N = row[5]\n",
    "CONNECTION_STRING = row[6]\n",
    "DRIVER = row[7]\n",
    "SOURCE_TABLES = row[8]\n",
    "TARGET_TABLE = row[9]\n",
    "TYPE_OF_JOIN = row[10]\n",
    "JOIN_COLUMN = row[11]\n",
    "USER_NAME_V = row[12]\n",
    "PASSWORD_V = row[13]\n",
    "\n",
    "print(ID_NUM)\n",
    "print(SOURCE_PATH)\n",
    "print(SOURCE_FILE_1)\n",
    "print(SOURCE_FILE_2)\n",
    "print(TARGET_FILE)\n",
    "print(DATABASE_N)\n",
    "print(CONNECTION_STRING)\n",
    "print(DRIVER)\n",
    "print(SOURCE_TABLES)\n",
    "print(TARGET_TABLE)\n",
    "print(TYPE_OF_JOIN)\n",
    "print(JOIN_COLUMN)\n",
    "print(USER_NAME_V)\n",
    "print(PASSWORD_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)\n",
    "\n",
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ JOIN_COLUMN + \" == source_df2.\"+ JOIN_COLUMN+ \",\" +\"'\"+ TYPE_OF_JOIN +\"')\"\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + CONNECTION_STRING + '\").option(\"dbtable\",\"' + TARGET_TABLE + '\").option(\"user\",\"' + USER_NAME_V +'\").option(\"password\",\"' + PASSWORD_V + '\").option(\"driver\",\"' + DRIVER + '\").save()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
